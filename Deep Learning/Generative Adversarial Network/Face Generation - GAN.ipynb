{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1551cfdc",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee01940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "import PIL\n",
    "import pdb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb #pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee735d29",
   "metadata": {},
   "source": [
    "# Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ba084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb - weight and bias\n",
    "def show(tensor, num=25, wandbactive=0, name=''):\n",
    "    data = tensor.detach().cpu()\n",
    "    grid = make_grid(data[:num], nrow=5).permute(1, 2, 0)\n",
    "    \n",
    "    #optional\n",
    "    if(wandbactive==1):\n",
    "        wandb.log({name:wandb.Image(grid.numpy().clip(0, 1))})\n",
    "        \n",
    "    plt.imshow(grid.clip(0, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3b999",
   "metadata": {},
   "source": [
    "# Parameters & Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/img_align_celeba/'\n",
    "checkpt_path ='./Checkpoints/'\n",
    "#critic = Discriminator\n",
    "epochs = 10000\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "z_dim=200\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "crit_cycles=5 #5 cycle train discriminator(critic) & 1 cycle train generator cuz discriminator is less powerful than generator (so need more training)\n",
    "gen_losses=[]\n",
    "crit_losses=[]\n",
    "wandbact = 1 # 1 - to visualize in wandb site (optional)\n",
    "last_epoch = 0 #contains last epoch saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6a40c",
   "metadata": {},
   "source": [
    "# Login & config wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place your wandb API generated from the site in a file 'wandb_API.txt' in the working directory\n",
    "f = open('wandb_API.txt')\n",
    "key = f.read()\n",
    "f.close()\n",
    "wandb.login(key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81712e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "exp_name = wandb.util.generate_id()\n",
    "myrun = wandb.init(\n",
    "        project='Face_GAN',\n",
    "        group=exp_name,\n",
    "        config={\n",
    "            'optimizer':'sgd',\n",
    "            'model':'wgan gp',\n",
    "            'epoch':'1000',\n",
    "            'batch_size':128\n",
    "        }\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c92fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c661b7",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "https://www.kaggle.com/jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300139fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, path, size=128, limit=10000):\n",
    "        self.sizes=[size, size]#width, height\n",
    "        items, labels=[], []\n",
    "        \n",
    "        for data in os.listdir(path)[:limit]:\n",
    "            item = os.path.join(path, data)\n",
    "            items.append(item)\n",
    "            labels.append(data)#labels not needed\n",
    "        self.items = items\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx): #PIL -> np -> tensor\n",
    "        data = PIL.Image.open(self.items[idx]).convert('RGB') #178x218\n",
    "        data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) #128x128x3\n",
    "        data = np.transpose(data, (2,0,1)).astype(np.float32, copy=False) # 3x128x128 for compatibility\n",
    "        data = torch.from_numpy(data).div(255) # scaling\n",
    "        return data, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d99f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(data_path, size=128, limit=10000)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "x, y = next(iter(dataloader))\n",
    "show(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd5087",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e2406",
   "metadata": {},
   "source": [
    "Conv2d : `(n+2*pad-ks)//stride + 1`<br>\n",
    "convTranspose2d : `(n-1)*stride - 2*padding + ks`\n",
    "\n",
    "- n : width or height\n",
    "- ks : kernel size\n",
    "- ConvTranspose2d : in_channels, out_channels, kernel_size, stride=1, padding=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c99ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=64, d_dim=16):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, d_dim*32, kernel_size=4, stride=1, padding=0), #200 ->512 ; 1x1 -> 4x4\n",
    "            # begin with 1 x 1 image with z_dim channels (decrease channel and increase size)\n",
    "            nn.BatchNorm2d(d_dim*32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*32, d_dim*16, kernel_size=4, stride=2, padding=1), #512 -> 256 ; 4x4 -> 8x8\n",
    "            nn.BatchNorm2d(d_dim*16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*16, d_dim*8, kernel_size=4, stride=2, padding=1), #256 -> 128 ; 8x8 -> 16x16\n",
    "            nn.BatchNorm2d(d_dim*8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*8, d_dim*4, kernel_size=4, stride=2, padding=1), #128 -> 64 ; 16x16 -> 32x32\n",
    "            nn.BatchNorm2d(d_dim*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*4, d_dim*2, kernel_size=4, stride=2, padding=1), #64 -> 32 ; 32x32 ->64x64\n",
    "            nn.BatchNorm2d(d_dim*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(d_dim*2, 3, kernel_size=4, stride=2, padding=1), #32 -> 3 ; 64x64 -> 128x128\n",
    "            nn.Tanh() # out range [-1, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1) #128(batch size) x 200(latent space) x 1(width) x 1(height)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise(num, z_dim, device=device):\n",
    "    return torch.randn(num, z_dim, device=device) # 128 x 200    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a5f5c",
   "metadata": {},
   "source": [
    "# Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, d_dim=16):\n",
    "        super(Critic, self).__init__()\n",
    "        self.crit = nn.Sequential(\n",
    "            nn.Conv2d(3, d_dim, 4, 2, 1), #128x128 -> 64x64 ; 3 -> 16\n",
    "            nn.InstanceNorm2d(d_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim, d_dim*2, 4, 2, 1), #64x64 -> 32x32 ; 16 -> 32\n",
    "            nn.InstanceNorm2d(d_dim*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim*2, d_dim*4, 4, 2, 1), #32x32 -> 16x16 ; 32 -> 64\n",
    "            nn.InstanceNorm2d(d_dim*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim*4, d_dim*8, 4, 2, 1), #16x16 -> 8x8; 64 -> 128\n",
    "            nn.InstanceNorm2d(d_dim*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim*8, d_dim*16, 4, 2, 1), #8x8 -> 4x4; 128->256\n",
    "            nn.InstanceNorm2d(d_dim*16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(d_dim*16, 1, 4, 1, 0) # 4x4 -> 1x1 ; 256->1\n",
    "        )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        #image : 128(batch)x3(channel)x128x128(w,h)\n",
    "        crit_pred = self.crit(image) #128(batch)x1(channel)x1x1(w,h)\n",
    "        return crit_pred.view(len(crit_pred), -1) # 128 x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20e386",
   "metadata": {},
   "source": [
    "# Initialize Weights (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c29840",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional, init your weights in different ways\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m,nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias,0)\n",
    "\n",
    "    if isinstance(m,nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias,0)\n",
    "\n",
    "##gen=gen.apply(init_weights)\n",
    "##crit=crit.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689125f9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "gen = Generator(z_dim).to(device)\n",
    "critic = Critic().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "critic_opt = torch.optim.Adam(critic.parameters(), lr=lr, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb\n",
    "if(wandbact==1):\n",
    "    wandb.watch(gen, log_freq=100)\n",
    "    wandb.watch(critic, log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1e0b2",
   "metadata": {},
   "source": [
    "# Gradient Penalty Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_penalty(real, fake, critic, alpha, gamma=10):\n",
    "    mix_images = real*alpha + fake*(1-alpha) #128(batch)x3(channel)x128x128(w,h)\n",
    "    mix_scores = critic(mix_images) #128x1\n",
    "    \n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = mix_images,\n",
    "        outputs = mix_scores,\n",
    "        grad_outputs = torch.ones_like(mix_scores),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0] #128x3x128x128\n",
    "    \n",
    "    gradient = gradient.view(len(gradient), -1) #128x 49152(128*128*3)\n",
    "    gradient_norm = gradient.norm(2, dim=1)# 2 - L2 norm only on 49152\n",
    "    grad_penalty = gamma*((gradient_norm-1)**2).mean()\n",
    "    \n",
    "    return grad_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08ed05",
   "metadata": {},
   "source": [
    "# Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chckpt(name):\n",
    "    torch.save({\n",
    "        'epoch':epoch,\n",
    "        'model_state_dict':gen.state_dict(),\n",
    "        'optimizer_state_dict':gen_opt.state_dict()\n",
    "    }, f\"{checkpt_path}G-{name}.pkl\")\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch':epoch,\n",
    "        'model_state_dict':critic.state_dict(),\n",
    "        'optimizer_state_dict':critic_opt.state_dict()\n",
    "    }, f\"{checkpt_path}Critic-{name}.pkl\")\n",
    "    \n",
    "    print(f\"Saved Checkpoint:\\n\\t Epoch : {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1f55c",
   "metadata": {},
   "source": [
    "# Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824bef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chckpt(name):\n",
    "    checkpoint = torch.load(f\"{checkpt_path}G-{name}.pkl\")\n",
    "    gen.load_state_dict(checkpoint['model_state_dict'])\n",
    "    gen_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    checkpoint = torch.load(f\"{checkpt_path}Critic-{name}.pkl\")\n",
    "    critic.load_state_dict(checkpoint['model_state_dict'])\n",
    "    critic_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    last_epoch = checkpoint['epoch']\n",
    "\n",
    "    print(f\"Checkpoint Loaded:\\n\\t Epoch : {last_epoch}\")\n",
    "    return last_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d2a0d",
   "metadata": {},
   "source": [
    "# Load From Previous Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_epoch = load_chckpt(\"Checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e90fa",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a098745",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pbar1 = tqdm(range(epochs))\n",
    "pbar1.n = last_epoch\n",
    "pbar1.refresh()\n",
    "for epoch in range(last_epoch,epochs):\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_batch_size = len(real) #128\n",
    "        real=real.to(device)\n",
    "        \n",
    "        #Critic\n",
    "        mean_critic_loss = 0\n",
    "        for i in range(crit_cycles): # 5 times critic 1 time generator\n",
    "            critic_opt.zero_grad()\n",
    "            \n",
    "            noise = gen_noise(cur_batch_size, z_dim, device)\n",
    "            fake = gen(noise)\n",
    "            critic_fake_pred = critic(fake.detach())\n",
    "            critic_real_pred = critic(real)\n",
    "            \n",
    "            alpha = torch.rand(len(real), 1,1,1, device=device, requires_grad=True) #128x1x1x1\n",
    "            grad_penalty = get_grad_penalty(real, fake.detach(), critic, alpha)\n",
    "            \n",
    "            crit_loss = critic_fake_pred.mean() - critic_real_pred.mean() + grad_penalty\n",
    "            \n",
    "            mean_critic_loss+=crit_loss.item() / crit_cycles\n",
    "            \n",
    "            crit_loss.backward(retain_graph=True)\n",
    "            critic_opt.step()\n",
    "        crit_losses+=[mean_critic_loss]\n",
    "        \n",
    "        #generator\n",
    "        gen_opt.zero_grad()\n",
    "        noise = gen_noise(cur_batch_size, z_dim, device)\n",
    "        fake = gen(noise)\n",
    "        critic_fake_pred = critic(fake)\n",
    "        \n",
    "        gen_loss = -critic_fake_pred.mean()\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        \n",
    "        gen_losses+=[gen_loss.item()]\n",
    "        \n",
    "    if(wandbact==1):\n",
    "        wandb.log({'Epoch':epoch, 'Critic Loss':mean_critic_loss, 'Generator Loss':gen_loss})\n",
    "\n",
    "    show(fake, wandbactive=1, name='fake')\n",
    "    show(real, wandbactive=1, name='real')\n",
    "\n",
    "    gen_mean = sum(gen_losses[-len(dataloader):]) / len(dataloader)\n",
    "    critic_mean = sum(crit_losses[-len(dataloader):]) / len(dataloader)\n",
    "\n",
    "    plt.plot(range(len(gen_losses)), torch.Tensor(gen_losses), label='Generator Loss')\n",
    "    plt.plot(range(len(gen_losses)), torch.Tensor(crit_losses), label='Critic Loss')\n",
    "    plt.ylim(-150, 150)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(f\"Epoch : {epoch}; Generator Loss : {gen_mean}; Critic Loss : {critic_mean}\\n\")\n",
    "\n",
    "    save_chckpt(\"Checkpoint\")\n",
    "        \n",
    "    pbar1.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f4dee",
   "metadata": {},
   "source": [
    "`10000 / 128 = 78.125 steps`<br>\n",
    "`50000 / 128 = 390.625 steps`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6f73f",
   "metadata": {},
   "source": [
    "# Generate New Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = gen_noise(batch_size, z_dim)\n",
    "fake = gen(noise)\n",
    "show(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake[4].detach().cpu().permute(1,2,0).squeeze().clip(0,1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
