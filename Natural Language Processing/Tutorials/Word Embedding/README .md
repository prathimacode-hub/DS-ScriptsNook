## PROJECT TITLE
# :dart: **Word Embedding Techniques**
Word embedding implements language modeling and feature extraction based techniques to map a word to vectors of real numbers.

<img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/Word-Vectors.png"
     alt="Markdown Monster icon"
     style="float: left; margin-right: 10px;" />
  ## GOAL
The goal of this project is brief understanding of Word Embedding Techniques
     
##  :page_facing_up: INTRODUCTION
What is word embedding with example?

Understanding the intuition behind word embedding creation with deep learning.For example, words like “mom” and “dad” should be closer together than the words “mom” and “ketchup” or “dad” and “butter”. Word embeddings are created using a neural network with one input layer, one hidden layer and one output layer

## What have I done?

1.Importing all the required libraries.

2.BRIEF EXPLANATION of  word embedding techniques.
  * TF-IDF Encoding
 
 * One Hot Encoding
 
 * Word2Vec
 
 * Continuous Bowl of Words(CBOW)
 
 * skip Gram
 
 * Bag of Words
 
 * Glove
 
 * Embedding Layer
 
5.Pratical Implementation of word embedding techniques.

4.Upload  the Jupyter Notebook file.

## :page_facing_up:  **BRIEF EXPLANATION**
In this I have cover various word embedding techniques.


 :pushpin: Binary Encoding
 ![image](https://user-images.githubusercontent.com/70129990/137106374-944c8906-9922-4dd0-a306-3f086f376bf8.png)

 
 :pushpin: TF-IDF Encoding
 ![image](https://user-images.githubusercontent.com/70129990/137106725-7e69d048-e04e-44a0-9ab0-fc602120e103.png)

 
 :pushpin: One Hot Encoding
 ![image](https://user-images.githubusercontent.com/70129990/137106524-97195f01-76b2-481a-9d51-c40538f7a7d8.png)

 
 :pushpin: Word2Vec
 
 :pushpin: Continuous Bowl of Words(CBOW)
 
 :pushpin: skip Gram
 
 ![image](https://user-images.githubusercontent.com/70129990/137114319-d76a8175-8baf-4a63-a0db-c5031b49ae44.png)

 :pushpin: Bag of Words
 
 :pushpin: Glove
 
 :pushpin: Embedding Layer
 
 ![image](https://user-images.githubusercontent.com/70129990/137105038-6283a232-9e18-4d00-b81e-0acf8a72c6f1.png)


# :bar_chart: **Pratical Implementation**

<code>Code</code> [See here](./word_embeddings.ipynb)
* Word Embedding Techniques using Embedding Layer in Keras.
* One Hot Encoding.
* Word2Vec.

 
## :key: LIBRARIES USED

* Pandas
* Numpy
* tensorflow
* keras
* sklearn
* nltk
* gensim

# :bulb:  CONCLUSION
We can use any one of the text feature extraction based on our project requirement. Because every method has their advantages  like a Bag-Of-Words suitable for text classification, TF-IDF is for document classification and if you want semantic relation between words then go with word2vec.


#  :thought_balloon: REFERENCES
https://machinelearningmastery.com/what-are-word-embeddings/

https://dataaspirant.com/word-embedding-techniques-nlp/













## Author
Code Contributed by Tanvi Deshmukh.
![MIT License](https://img.shields.io/badge/Made_With_Jupyter-2CA5E0?style=for-the-badge_Color=whit)

  
