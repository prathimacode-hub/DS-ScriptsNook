## PROJECT TITLE
# :pushpin: **Word Embeddings Techniques**

![alt text](https://www.google.com/imgres?imgurl=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1838%2F1*jpnKO5X0Ii8PVdQYFO2z1Q.png&imgrefurl=https%3A%2F%2Ftowardsdatascience.com%2Fword-embedding-with-word2vec-and-fasttext-a209c1d3e12c&tbnid=NbAthHQT97GalM&vet=12ahUKEwin_5CC1cLzAhXZm0sFHWD6B1gQMygregUIARCIAg..i&docid=Pet2KX6omOSsjM&w=1748&h=952&q=word%20embedding%20techniques%20image&ved=2ahUKEwin_5CC1cLzAhXZm0sFHWD6B1gQMygregUIARCIAg)
## INTRODUCTION
We will learn about various word embedding techniques.

## page_facing_up:  **BRIEF EXPLANATION**
In this I have cover various word embedding techniques.

# dart: **Binary Encoding**.
# dart: **TF-IDF Encoding.**
# dart: **One Hot Encoding**
# dart: **Word2Vec**
# dart: **Continuous Bowl of Words(CBOW)**
# dart: **Skip Gram**
# dart: **Bag of Words**
# dart: **GloVe**
# dart: **Embedding Layer**

# page_facing_up: **Pratical Implementation**

* Word Embedding Techniques using Embedding Layer in Keras.
* One Hot Encoding.
* Word2Vec.
 
## LIBRARIES USED

* Pandas
* Numpy
* tensorflow
* keras
* sklearn
* nltk
* gensim

#  CONCLUSION
We can use any one of the text feature extraction based on our project requirement. Because every method has their advantages  like a Bag-Of-Words suitable for text classification, TF-IDF is for document classification and if you want semantic relation between words then go with word2vec.
# REFERENCES
https://machinelearningmastery.com/what-are-word-embeddings/

https://dataaspirant.com/word-embedding-techniques-nlp/













## Author
Code Contributed by Tanvi Deshmukh.
![MIT License](https://img.shields.io/badge/Made_With_Jupyter-2CA5E0?style=for-the-badge_Color=whit)

  
