## PROJECT TITLE
# :dart: **Word Embeddings Techniques**

<img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/Word-Vectors.png"
     alt="Markdown Monster icon"
     style="float: left; margin-right: 10px;" />
##  :page_facing_up: INTRODUCTION
We will learn about various word embedding techniques.

## :page_facing_up:  **BRIEF EXPLANATION**
In this I have cover various word embedding techniques.


 :pushpin: Binary Encoding
 
 :pushpin: TF-IDF Encoding
 
 :pushpin: One Hot Encoding
 
 :pushpin: Word2Vec
 
 :pushpin: Continuous Bowl of Words(CBOW)
 
 :pushpin: kip Gram
 
 :pushpin: Bag of Words
 
 :pushpin: GloVe
 
 :pushpin: Embedding Layer

# :bar_chart: **Pratical Implementation**

* Word Embedding Techniques using Embedding Layer in Keras.
* One Hot Encoding.
* Word2Vec.
 
## :key: LIBRARIES USED

* Pandas
* Numpy
* tensorflow
* keras
* sklearn
* nltk
* gensim

# :bulb:  CONCLUSION
We can use any one of the text feature extraction based on our project requirement. Because every method has their advantages  like a Bag-Of-Words suitable for text classification, TF-IDF is for document classification and if you want semantic relation between words then go with word2vec.


#  :thought_balloon: REFERENCES
https://machinelearningmastery.com/what-are-word-embeddings/

https://dataaspirant.com/word-embedding-techniques-nlp/













## Author
Code Contributed by Tanvi Deshmukh.
![MIT License](https://img.shields.io/badge/Made_With_Jupyter-2CA5E0?style=for-the-badge_Color=whit)

  
