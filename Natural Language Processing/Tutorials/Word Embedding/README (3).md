## PROJECT TITLE
# :pushpin: **Word Embeddings Techniques**


## INTRODUCTION
We will learn about various word embedding techniques.

## BRIEF EXPLANATION
In this I have cover various word embedding techniques.

* Binary Encoding.
* TF-IDF Encoding.
* One Hot Encoding
* Word2Vec
* Continuous Bowl of Words(CBOW)
* Skip Gram
* Bag of Words
* GloVe
* Embedding Layer

# Pratical Implementation

* Word Embedding Techniques using Embedding Layer in Keras.
* One Hot Encoding.
* Word2Vec.
 
## LIBRARIES USED

* Pandas
* Numpy
* tensorflow
* keras
* sklearn
* nltk
* gensim

#  CONCLUSION
We can use any one of the text feature extraction based on our project requirement. Because every method has their advantages  like a Bag-Of-Words suitable for text classification, TF-IDF is for document classification and if you want semantic relation between words then go with word2vec.
# REFERENCES
https://machinelearningmastery.com/what-are-word-embeddings/

https://dataaspirant.com/word-embedding-techniques-nlp/













## Author
Code Contributed by Tanvi Deshmukh.
![MIT License](https://img.shields.io/badge/Made_With_Jupyter-2CA5E0?style=for-the-badge_Color=whit)

  
