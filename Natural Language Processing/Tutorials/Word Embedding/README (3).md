## PROJECT TITLE
Word Embeddings Techniques

<img src = "https://www.google.com/imgres?imgurl=https%3A%2F%2Fcdn.analyticsvidhya.com%2Fwp-content%2Fuploads%2F2019%2F11%2FWord-Vectors.png&imgrefurl=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2020%2F08%2Ftop-4-sentence-embedding-techniques-using-python%2F&tbnid=FFYTHb0sqhoRfM&vet=12ahUKEwjr1qyr0sLzAhWhnksFHX8_DRwQMygCegUIARC1AQ..i&docid=iSq6gLsGZubvSM&w=1505&h=527&q=word%20embedding%20techniques%20image&ved=2ahUKEwjr1qyr0sLzAhWhnksFHX8_DRwQMygCegUIARC1AQ">

## INTRODUCTION
We will learn about various word embedding techniques.

## BRIEF EXPLANATION
In this I have cover various word embedding techniques.

* Binary Encoding.
* TF-IDF Encoding.
* One Hot Encoding
* Word2Vec
* Continuous Bowl of Words(CBOW)
* Skip Gram
* Bag of Words
* GloVe
* Embedding Layer

# Pratical Implementation

* Word Embedding Techniques using Embedding Layer in Keras.
* One Hot Encoding.
* Word2Vec.
 
## LIBRARIES USED

* Pandas
* Numpy
* tensorflow
* keras
* sklearn
* nltk
* gensim

#  CONCLUSION
We can use any one of the text feature extraction based on our project requirement. Because every method has their advantages  like a Bag-Of-Words suitable for text classification, TF-IDF is for document classification and if you want semantic relation between words then go with word2vec.
# REFERENCES
https://machinelearningmastery.com/what-are-word-embeddings/

https://dataaspirant.com/word-embedding-techniques-nlp/













## Author
Code Contributed by Tanvi Deshmukh.
![MIT License](https://img.shields.io/badge/Made_With_Jupyter-2CA5E0?style=for-the-badge_Color=whit)

  